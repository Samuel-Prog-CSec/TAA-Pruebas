{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036da9ce",
   "metadata": {},
   "source": [
    "# Clustering de movimiento humano\n",
    "\n",
    "## Descripción del proyecto\n",
    "Este notebook implementa un pipeline completo de clustering de datos de movimiento humano siguiendo el enunciado del proyecto:\n",
    "\n",
    "1. **Carga y combinación de datos** multi-archivo por participante\n",
    "2. **Transformación a coordenadas relativas** (respecto a pelvis) para análisis biomecánico\n",
    "3. **Segmentación temporal** (ventanas de 4 segundos)\n",
    "4. **Normalización** (por participante y espacial)\n",
    "5. **Extracción de características** biomecánicas relevantes\n",
    "6. **Clustering** con algoritmo justificado\n",
    "7. **Interpretación** biomecánica de los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab34409",
   "metadata": {},
   "source": [
    "## 1. Configuración inicial e imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad5735d",
   "metadata": {},
   "source": [
    "### Metodología y justificación\n",
    "\n",
    "**Objetivo**: preparar el entorno computacional con todas las librerías necesarias para el análisis de clustering de movimiento humano.\n",
    "\n",
    "**Métodos aplicados**:\n",
    "- **Librerías de clustering**: KMeans, AgglomerativeClustering y DBSCAN para comparar diferentes enfoques algorítmicos\n",
    "- **Preprocesamiento**: StandardScaler para normalización, fundamental en clustering por la sensibilidad a escalas\n",
    "- **Reducción dimensional**: PCA y t-SNE para visualización e interpretación de resultados\n",
    "- **Métricas de evaluación**: Silhouette, Calinski-Harabasz y Davies-Bouldin para validación objetiva\n",
    "\n",
    "**Justificación**:\n",
    "- El clustering de datos biomecánicos requiere múltiples algoritmos para encontrar el más adecuado a los patrones de movimiento\n",
    "- La normalización es crítica porque las características pueden tener diferentes unidades y rangos\n",
    "- Las técnicas de visualización permiten interpretar clusters en espacios de alta dimensionalidad\n",
    "\n",
    "**Resultado esperado**: entorno preparado para realizar análisis robusto y reproducible de patrones de movimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2457d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports básicos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Clustering y análisis\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Configuración\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Todas las librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d9ce44",
   "metadata": {},
   "source": [
    "## 2. Configuración de rutas y parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eafb82",
   "metadata": {},
   "source": [
    "### Metodología y justificación\n",
    "\n",
    "**Objetivo**: establecer parámetros fundamentales que determinarán la calidad y validez del análisis biomecánico.\n",
    "\n",
    "**Parámetros críticos definidos**:\n",
    "- **Frecuencia de muestreo (60 Hz)**: segun enunciado\n",
    "- **Ventana temporal (4 segundos)**: según enunciado, comprende 1-2 ciclos de marcha completos, esencial para capturar patrones cíclicos\n",
    "- **Articulaciones reales**: lista validada experimentalmente, evita articulaciones artificiales o con datos faltantes\n",
    "\n",
    "**Justificación biomecánica**:\n",
    "- 4 segundos capturan la variabilidad intra-ciclo e inter-ciclo de la marcha\n",
    "- La selección de articulaciones reales garantiza datos consistentes y biomecánicamente relevantes\n",
    "\n",
    "**Resultado esperado**: parámetros optimizados que aseguran la validez científica del análisis posterior y la comparabilidad con literatura biomecánica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b8b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de rutas\n",
    "PROJECT_ROOT = Path(r\".\")\n",
    "DATASET_PATH = PROJECT_ROOT / \"dataset\" / \"data\"\n",
    "PARTICIPANTS_FILE = PROJECT_ROOT / \"dataset\" / \"participants.xlsx\"\n",
    "\n",
    "# Parámetros del pipeline\n",
    "SAMPLING_RATE = 60  # Hz\n",
    "WINDOW_SIZE_SECONDS = 4  # segundos (según enunciado)\n",
    "WINDOW_SIZE_SAMPLES = SAMPLING_RATE * WINDOW_SIZE_SECONDS\n",
    "\n",
    "# Articulaciones REALES detectadas en el dataset (basado en análisis previo)\n",
    "REAL_JOINTS = [\n",
    "    'L3', 'L5', 'T12', 'T8', 'footLeft', 'footRight', 'forearmLeft', 'forearmRight', \n",
    "    'handLeft', 'handRight', 'head', 'lowerLegLeft', 'lowerLegRight', 'neck', 'pelvis', \n",
    "    'shoulderLeft', 'shoulderRight', 'toeLeft', 'toeRight', 'upperArmLeft', 'upperArmRight', \n",
    "    'upperLegLeft', 'upperLegRight'\n",
    "]\n",
    "\n",
    "print(f\"Ruta del dataset: {DATASET_PATH}\")\n",
    "print(f\"Tamaño de ventana: {WINDOW_SIZE_SECONDS}s ({WINDOW_SIZE_SAMPLES} muestras)\")\n",
    "print(f\"Articulaciones reales en el dataset: {len(REAL_JOINTS)}\")\n",
    "print(f\"Articulación de referencia: pelvis (confirmada)\")\n",
    "\n",
    "# Verificar coherencia de parámetros\n",
    "print(f\"\\nParámetros verificados:\")\n",
    "print(f\"   Frecuencia de muestreo: {SAMPLING_RATE} Hz\")\n",
    "print(f\"   Muestras por ventana: {WINDOW_SIZE_SAMPLES}\")\n",
    "print(f\"   Ejes por articulación: 3 (x, y, z)\")\n",
    "print(f\"   Total columnas de posición esperadas: {len(REAL_JOINTS) * 3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bd00a5",
   "metadata": {},
   "source": [
    "## 3. Carga y combinación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef07ab4",
   "metadata": {},
   "source": [
    "### Metodología y justificación\n",
    "\n",
    "**Objetivo**: consolidar datos multi-archivo por participante en un dataset unificado manteniendo trazabilidad de origen.\n",
    "\n",
    "**Metodología aplicada**:\n",
    "- **Carga iterativa**: procesamiento por participante evitando carga masiva en memoria\n",
    "- **Combinación vertical**: concatenación de múltiples recordings por participante preservando estructura temporal\n",
    "- **Manejo de errores**: gestión robusta de archivos corruptos o faltantes\n",
    "- **Etiquetado**: identificación de participante y recording para análisis posterior\n",
    "\n",
    "**Justificación técnica**:\n",
    "- El diseño experimental genera múltiples sesiones por participante que deben consolidarse\n",
    "- La estructura jerárquica (participante → recordings) refleja la naturaleza del experimento\n",
    "- El manejo de errores previene fallos por datos inconsistentes o archivos dañados\n",
    "\n",
    "**Consideraciones biomecánicas**:\n",
    "- Cada recording representa una sesión independiente de marcha\n",
    "- La consolidación permite analizar patrones intra-participante e inter-participante\n",
    "- La preservación del orden temporal es crucial para análisis de series temporales\n",
    "\n",
    "**Resultado esperado**: Dataset estructurado con ~23 participantes y múltiples recordings, listo para análisis exploratorio y transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a902cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_participant_data(participant_dir):\n",
    "    \"\"\"Carga y combina todos los archivos CSV de un participante\"\"\"\n",
    "    csv_files = list(participant_dir.glob(\"rec_*.csv\"))\n",
    "    if not csv_files:\n",
    "        return None\n",
    "    \n",
    "    data_frames = []\n",
    "    for file_path in sorted(csv_files):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['recording'] = file_path.stem  # rec_0, rec_1, etc.\n",
    "            data_frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error cargando {file_path}: {e}\")\n",
    "    \n",
    "    if data_frames:\n",
    "        combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "        return combined_df\n",
    "    return None\n",
    "\n",
    "# Cargar datos de todos los participantes\n",
    "all_data = []\n",
    "participants = []\n",
    "\n",
    "for participant_dir in sorted(DATASET_PATH.glob(\"sub_*\")):\n",
    "    participant_id = participant_dir.name\n",
    "    participant_data = load_participant_data(participant_dir)\n",
    "    \n",
    "    if participant_data is not None:\n",
    "        participant_data['participant'] = participant_id\n",
    "        all_data.append(participant_data)\n",
    "        participants.append(participant_id)\n",
    "        print(f\"{participant_id}: {len(participant_data)} frames\")\n",
    "    else:\n",
    "        print(f\"{participant_id}: No se pudieron cargar datos\")\n",
    "\n",
    "# Combinar todos los datos\n",
    "if all_data:\n",
    "    complete_data = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"\\nDataset completo: {len(complete_data)} frames de {len(participants)} participantes\")\n",
    "    print(f\"Columnas: {list(complete_data.columns)}\")\n",
    "else:\n",
    "    raise ValueError(\"No se pudieron cargar datos de ningún participante\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1127e6",
   "metadata": {},
   "source": [
    "## 4. Análisis exploratorio y detección de articulaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e7f1d8",
   "metadata": {},
   "source": [
    "### Metodología y justificación\n",
    "\n",
    "**Objetivo**: validar la integridad del dataset y identificar las articulaciones realmente disponibles para el análisis biomecánico.\n",
    "\n",
    "**Metodología exploratoria**:\n",
    "- **Detección automática de articulaciones**: identificación de columnas de posición (x,y,z) evitando dependencia de listas predefinidas\n",
    "- **Análisis de completitud**: evaluación de valores faltantes por articulación para identificar datos problemáticos\n",
    "- **Validación de estructura**: verificación de que el dataset contiene la información necesaria para análisis biomecánico\n",
    "\n",
    "**Justificación científica**:\n",
    "- Los sistemas de captura de movimiento pueden fallar en ciertas articulaciones, especialmente distales\n",
    "- El análisis exploratorio previene errores downstream causados por datos faltantes o inconsistentes\n",
    "- La identificación empírica de articulaciones garantiza robustez frente a variaciones en nomenclatura\n",
    "\n",
    "**Consideraciones técnicas**:\n",
    "- Se excluyen explícitamente variables de velocidad (`vel_x`, `vel_y`, `vel_z`) para focus en posición\n",
    "- La ordenación alfabética facilita la interpretación y reproducibilidad\n",
    "- El análisis de missing values informa decisiones de preprocesamiento\n",
    "\n",
    "**Resultado esperado**: lista validada de articulaciones disponibles y evaluación de calidad de datos para proceder con transformaciones biomecánicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar columnas de posición (x, y, z) disponibles\n",
    "# Usar las columnas reales del dataset (no las predefinidas)\n",
    "position_cols = [col for col in complete_data.columns \n",
    "                if col.endswith(('_x', '_y', '_z')) and col not in ['vel_x', 'vel_y', 'vel_z']]\n",
    "\n",
    "# Extraer articulaciones únicas disponibles\n",
    "available_joints = list(set([col.rsplit('_', 1)[0] for col in position_cols]))\n",
    "available_joints.sort()\n",
    "\n",
    "print(f\"Articulaciones disponibles en el dataset: {len(available_joints)}\")\n",
    "print(f\"   {available_joints}\")\n",
    "print(f\"Total de columnas de posición: {len(position_cols)}\")\n",
    "\n",
    "# Verificar valores faltantes\n",
    "missing_summary = complete_data[position_cols].isnull().sum()\n",
    "print(f\"\\nValores faltantes:\")\n",
    "print(f\"   Total: {missing_summary.sum()}\")\n",
    "if missing_summary.sum() > 0:\n",
    "    print(f\"   Por columna (top 5): {missing_summary.nlargest(5).to_dict()}\")\n",
    "else:\n",
    "    print(f\"   ¡Excelente! No hay valores faltantes en las coordenadas\")\n",
    "\n",
    "# Estadísticas básicas\n",
    "print(f\"\\nEstadísticas del dataset:\")\n",
    "print(f\"   Participantes únicos: {complete_data['participant'].nunique()}\")\n",
    "print(f\"   Grabaciones por participante: {complete_data.groupby('participant')['recording'].nunique().describe()}\")\n",
    "print(f\"   Frames por participante: {complete_data.groupby('participant').size().describe()}\")\n",
    "\n",
    "# Mostrar algunas columnas de ejemplo\n",
    "print(f\"\\nEjemplo de columnas de posición:\")\n",
    "print(f\"   {position_cols[:10]} ... (y {len(position_cols)-10} más)\")\n",
    "\n",
    "# Verificar si tenemos articulación de referencia (pelvis)\n",
    "reference_candidates = ['pelvis', 'hip', 'midhip']\n",
    "reference_found = None\n",
    "for candidate in reference_candidates:\n",
    "    if any(candidate.lower() in joint.lower() for joint in available_joints):\n",
    "        reference_found = candidate\n",
    "        break\n",
    "\n",
    "print(f\"\\nArticulación de referencia encontrada: {reference_found}\")\n",
    "if not reference_found:\n",
    "    print(f\"    No se encontró pelvis explícita, se calculará desde otras articulaciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9e3668",
   "metadata": {},
   "source": [
    "## 5. Transformación a coordenadas relativas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbe3064",
   "metadata": {},
   "source": [
    "### Metodología y justificación\n",
    "\n",
    "**Objetivo**: convertir coordenadas absolutas del laboratorio a coordenadas relativas respecto a la pelvis, eliminando efectos de traslación corporal.\n",
    "\n",
    "**Metodología biomecánica**:\n",
    "- **Articulación de referencia**: pelvis como origen anatómico, estándar en análisis de marcha\n",
    "- **Transformación punto a punto**: resta vectorial (articulación - pelvis) preservando dinámicas relativas\n",
    "- **Preservación temporal**: mantenimiento del orden temporal para análisis de series de tiempo\n",
    "\n",
    "**Justificación científica**:\n",
    "- **Elimina variabilidad de posición**: las diferencias de altura, posición inicial, y deriva del sistema quedan neutralizadas\n",
    "- **Focus en patrones motores**: se preservan únicamente los movimientos relativos entre segmentos corporales\n",
    "- **Estándar biomecánico**: protocolo establecido en análisis de marcha clínica y deportiva\n",
    "\n",
    "**Ventajas para clustering**:\n",
    "- **Normalización espacial**: los clusters reflejan patrones de coordinación, no diferencias antropométricas\n",
    "- **Reducción de ruido**: eliminación de artefactos de posicionamiento y calibración del sistema\n",
    "- **Comparabilidad inter-sujeto**: participantes de diferentes estaturas se vuelven comparables\n",
    "\n",
    "**Resultado esperado**: Dataset con coordenadas relativas que capturan exclusivamente patrones de coordinación intermuscular, óptimo para identificar estrategias motoras mediante clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781efb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar la articulación de referencia (pelvis/cadera)\n",
    "reference_joint = 'pelvis'  # Ya sabemos que existe del análisis anterior\n",
    "\n",
    "print(f\"Articulación de referencia: {reference_joint}\")\n",
    "\n",
    "# Verificar que existe\n",
    "if f\"{reference_joint}_x\" not in complete_data.columns:\n",
    "    raise ValueError(f\"No se encontró la articulación de referencia '{reference_joint}' en el dataset\")\n",
    "\n",
    "# Crear coordenadas relativas (para análisis biomecánico)\n",
    "relative_data = complete_data.copy()\n",
    "\n",
    "for joint in available_joints:\n",
    "    if joint != reference_joint:\n",
    "        for axis in ['x', 'y', 'z']:\n",
    "            joint_col = f\"{joint}_{axis}\"\n",
    "            ref_col = f\"{reference_joint}_{axis}\"\n",
    "            \n",
    "            if joint_col in relative_data.columns and ref_col in relative_data.columns:\n",
    "                relative_data[f\"{joint}_rel_{axis}\"] = (relative_data[joint_col] - \n",
    "                                                       relative_data[ref_col])\n",
    "\n",
    "# Columnas de posición relativa\n",
    "relative_position_cols = [col for col in relative_data.columns if col.endswith('_rel_x') or \n",
    "                         col.endswith('_rel_y') or col.endswith('_rel_z')]\n",
    "\n",
    "print(f\"Coordenadas relativas creadas: {len(relative_position_cols)} columnas\")\n",
    "print(f\"   Ejemplo: {relative_position_cols[:5]}\")\n",
    "\n",
    "# Verificar que las coordenadas relativas se crearon correctamente\n",
    "print(f\"\\nVerificación de coordenadas relativas:\")\n",
    "print(f\"   Articulaciones transformadas: {len(relative_position_cols)//3}\")\n",
    "print(f\"   Total de ejes (x, y, z): {len(relative_position_cols)}\")\n",
    "\n",
    "# Mostrar estadísticas de algunas coordenadas relativas\n",
    "sample_relative_cols = relative_position_cols[:6]  # Primeras 6 para ejemplo\n",
    "for col in sample_relative_cols:\n",
    "    values = relative_data[col].dropna()\n",
    "    print(f\"   {col}: media={values.mean():.3f}, std={values.std():.3f}, rango=[{values.min():.3f}, {values.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88326598",
   "metadata": {},
   "source": [
    "## 6. Segmentación temporal (ventanas de 4 segundos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8578dc3b",
   "metadata": {},
   "source": [
    "### Metodología y justificación\n",
    "\n",
    "**Objetivo**: segmentar la señal continua de movimiento en ventanas temporales de 4 segundos que capturen unidades funcionales de marcha.\n",
    "\n",
    "**Metodología de ventaneo**:\n",
    "- **Tamaño de ventana**: 4 segundos según especificación del enunciado\n",
    "- **Estrategia de solapamiento**: sin solapamiento (step_size = window_size) para independencia estadística\n",
    "- **Procesamiento por participante**: segmentación independiente por sujeto preservando continuidad temporal intra-sujeto\n",
    "\n",
    "**Justificación biomecánica**:\n",
    "- **Duración funcional**: 4 segundos incluyen 1-2 ciclos completos de marcha (ciclo ~1.0-1.2s en adultos sanos)\n",
    "- **Captura de variabilidad**: período suficiente para observar variaciones intra-ciclo e inicio de variaciones inter-ciclo\n",
    "- **Estabilidad de patrones**: ventana lo suficientemente larga para estabilizar características biomecánicas\n",
    "\n",
    "**Consideraciones técnicas**:\n",
    "- **Sin solapamiento**: evita dependencias entre ventanas, crucial para validez estadística del clustering\n",
    "- **Preservación de secuencialidad**: mantiene orden temporal dentro de cada participante\n",
    "- **Robustez a diferentes duraciones**: función adaptable a diferentes duraciones de recording por participante\n",
    "\n",
    "**Resultado esperado**: conjunto de ventanas independientes, cada una representando un segmento de marcha autocontenido, listo para extracción de características biomecánicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac74ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(data, window_size_samples, step_size=None):\n",
    "    \"\"\"Crear ventanas deslizantes de los datos\"\"\"\n",
    "    if step_size is None:\n",
    "        step_size = window_size_samples  # Sin solapamiento\n",
    "    \n",
    "    windows = []\n",
    "    \n",
    "    for participant in data['participant'].unique():\n",
    "        participant_data = data[data['participant'] == participant].copy()\n",
    "        # Resetear índice para ordenamiento correcto\n",
    "        participant_data = participant_data.reset_index(drop=True)\n",
    "        \n",
    "        for recording in participant_data['recording'].unique():\n",
    "            recording_data = participant_data[participant_data['recording'] == recording].copy()\n",
    "            recording_data = recording_data.reset_index(drop=True)\n",
    "            \n",
    "            # Crear ventanas para esta grabación\n",
    "            for start_idx in range(0, len(recording_data) - window_size_samples + 1, step_size):\n",
    "                end_idx = start_idx + window_size_samples\n",
    "                window = recording_data.iloc[start_idx:end_idx].copy()\n",
    "                \n",
    "                # Añadir metadatos de la ventana\n",
    "                window['window_id'] = len(windows)\n",
    "                window['window_start_frame'] = start_idx\n",
    "                \n",
    "                windows.append(window)\n",
    "    \n",
    "    return windows\n",
    "\n",
    "# Crear ventanas de 4 segundos\n",
    "print(f\"Creando ventanas de {WINDOW_SIZE_SECONDS}s ({WINDOW_SIZE_SAMPLES} muestras)...\")\n",
    "windows = create_windows(relative_data, WINDOW_SIZE_SAMPLES)\n",
    "\n",
    "print(f\"Ventanas creadas: {len(windows)}\")\n",
    "\n",
    "# Estadísticas de ventanas por participante\n",
    "window_counts = {}\n",
    "for window in windows:\n",
    "    participant = window['participant'].iloc[0]\n",
    "    if participant not in window_counts:\n",
    "        window_counts[participant] = 0\n",
    "    window_counts[participant] += 1\n",
    "\n",
    "print(f\"Ventanas por participante:\")\n",
    "for participant, count in sorted(window_counts.items()):\n",
    "    print(f\"   {participant}: {count} ventanas\")\n",
    "\n",
    "# Verificar que tenemos suficientes ventanas\n",
    "if len(windows) < 50:\n",
    "    print(f\"ADVERTENCIA: Pocas ventanas ({len(windows)}). Considerando reducir tamaño de ventana o aumentar solapamiento.\")\n",
    "else:\n",
    "    print(f\"Número adecuado de ventanas para clustering\")\n",
    "\n",
    "# Mostrar estadísticas generales\n",
    "print(f\"\\nEstadísticas de ventanas:\")\n",
    "window_counts_values = list(window_counts.values())\n",
    "print(f\"   Total de ventanas: {len(windows)}\")\n",
    "print(f\"   Promedio por participante: {np.mean(window_counts_values):.1f}\")\n",
    "print(f\"   Min-Max por participante: {min(window_counts_values)}-{max(window_counts_values)}\")\n",
    "print(f\"   Duración total por ventana: {WINDOW_SIZE_SECONDS}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc147eb7",
   "metadata": {},
   "source": [
    "## 7. Extracción de características biomecánicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a89d11",
   "metadata": {},
   "source": [
    "### Metodología y justificación\n",
    "\n",
    "**Objetivo**: extraer características biomecánicamente relevantes que capturen patrones de coordinación motora en cada ventana temporal.\n",
    "\n",
    "**Categorías de características extraídas**:\n",
    "\n",
    "1. **Características de posición** (coordenadas relativas):\n",
    "   - **Media**: posición promedio de cada articulación respecto a pelvis (centroide de movimiento)\n",
    "   - **Desviación estándar**: amplitud de movimiento en cada eje\n",
    "   - **Rango**: excursión máxima-mínima, indicador de ROM (Range of Motion)\n",
    "\n",
    "2. **Características de velocidad** (coordenadas relativas):\n",
    "   - **Velocidad promedio**: indicador de velocidad general del movimiento\n",
    "   - **Variabilidad de velocidad**: smoothness y consistencia del movimiento\n",
    "   - **Velocidad máxima**: picos de velocidad, relacionados con fases dinámicas\n",
    "\n",
    "3. **Características de distancias inter-articulares** (coordenadas absolutas):\n",
    "   - **Distancias 3D entre articulaciones clave**: ancho de hombros, separación de manos, etc.\n",
    "   - **Nota técnica**: Las distancias se calculan sobre coordenadas absolutas porque representan medidas antropométricas invariantes que no deben relativizarse respecto a la pelvis\n",
    "\n",
    "4. **Características de variabilidad temporal**:\n",
    "   - **Coeficiente de variación**: consistencia relativa del movimiento\n",
    "   - **Diferencias frame-a-frame**: indicador de suavidad (jerk implícito)\n",
    "\n",
    "**Justificación biomecánica**:\n",
    "- **Posición relativa**: captura estrategias espaciales independientes de la posición global del cuerpo\n",
    "- **Velocidad relativa**: refleja dinámicas motoras sin sesgo de traslación corporal\n",
    "- **Distancias absolutas**: preservan información antropométrica y postural relevante (ej: ancho de hombros, extensión de brazos)\n",
    "- **Variabilidad**: indica control motor, fatiga, y patrones de coordinación\n",
    "\n",
    "**Relevancia para clustering**:\n",
    "- Estas características son estándar en análisis biomecánico clínico\n",
    "- Capturan tanto aspectos cinemáticos como de control motor\n",
    "- Combinan información relativa (movimientos) y absoluta (configuración corporal)\n",
    "- Permiten identificar diferencias inter-individuales en estrategias motoras\n",
    "\n",
    "**Resultado esperado**: vector de características biomecánicamente interpretables que represente comprehensivamente la cinemática de cada ventana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff30685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_biomechanical_features(window_data, position_cols):\n",
    "    \"\"\"Extraer características biomecánicas de una ventana de datos\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # PARTE 1: Características de coordenadas RELATIVAS (respecto a pelvis)\n",
    "    # Para cada articulación y eje en coordenadas relativas\n",
    "    for col in position_cols:\n",
    "        if col in window_data.columns:\n",
    "            values = window_data[col].dropna()\n",
    "            \n",
    "            if len(values) > 0:\n",
    "                # Características estadísticas básicas\n",
    "                features[f\"{col}_mean\"] = values.mean()\n",
    "                features[f\"{col}_std\"] = values.std()\n",
    "                features[f\"{col}_min\"] = values.min()\n",
    "                features[f\"{col}_max\"] = values.max()\n",
    "                features[f\"{col}_range\"] = values.max() - values.min()\n",
    "                \n",
    "                # Características de movimiento\n",
    "                if len(values) > 1:\n",
    "                    velocity = np.diff(values)\n",
    "                    features[f\"{col}_velocity_mean\"] = np.mean(np.abs(velocity))\n",
    "                    features[f\"{col}_velocity_std\"] = np.std(velocity)\n",
    "                    \n",
    "                    if len(velocity) > 1:\n",
    "                        acceleration = np.diff(velocity)\n",
    "                        features[f\"{col}_acceleration_mean\"] = np.mean(np.abs(acceleration))\n",
    "                        features[f\"{col}_acceleration_std\"] = np.std(acceleration)\n",
    "    \n",
    "    # PARTE 2: Características de distancias 3D usando coordenadas ABSOLUTAS\n",
    "    # (Estas representan medidas antropométricas que no deben relativizarse)\n",
    "    joint_pairs = [\n",
    "        ('shoulderRight', 'shoulderLeft'),  # Ancho de hombros\n",
    "        ('upperLegRight', 'upperLegLeft'),  # Ancho de caderas\n",
    "        ('handRight', 'handLeft'),          # Separación de manos\n",
    "        ('footRight', 'footLeft'),          # Separación de pies (base de apoyo)\n",
    "        ('head', 'neck'),                   # Distancia cabeza-cuello\n",
    "    ]\n",
    "    \n",
    "    for joint1, joint2 in joint_pairs:\n",
    "        # Usar coordenadas ABSOLUTAS para distancias inter-articulares\n",
    "        abs_cols1 = [f\"{joint1}_x\", f\"{joint1}_y\", f\"{joint1}_z\"]\n",
    "        abs_cols2 = [f\"{joint2}_x\", f\"{joint2}_y\", f\"{joint2}_z\"]\n",
    "        \n",
    "        if all(col in window_data.columns for col in abs_cols1 + abs_cols2):\n",
    "            try:\n",
    "                dist_3d = np.sqrt(\n",
    "                    (window_data[f\"{joint1}_x\"] - window_data[f\"{joint2}_x\"])**2 +\n",
    "                    (window_data[f\"{joint1}_y\"] - window_data[f\"{joint2}_y\"])**2 +\n",
    "                    (window_data[f\"{joint1}_z\"] - window_data[f\"{joint2}_z\"])**2\n",
    "                )\n",
    "                features[f\"dist_{joint1}_{joint2}_mean\"] = dist_3d.mean()\n",
    "                features[f\"dist_{joint1}_{joint2}_std\"] = dist_3d.std()\n",
    "            except Exception as e:\n",
    "                # Si hay algún error, simplemente omitir esta característica\n",
    "                pass\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extraer características de todas las ventanas usando coordenadas relativas\n",
    "print(\"Extrayendo características biomecánicas...\")\n",
    "print(f\"  - Coordenadas RELATIVAS (respecto a pelvis): {len(relative_position_cols)} columnas de {len(relative_position_cols)//3} articulaciones\")\n",
    "print(f\"  - Distancias inter-articulares: coordenadas ABSOLUTAS (medidas antropométricas)\")\n",
    "\n",
    "features_list = []\n",
    "for i, window in enumerate(windows):\n",
    "    if i % 50 == 0:\n",
    "        print(f\"   Procesando ventana {i+1}/{len(windows)}\")\n",
    "    \n",
    "    # Usar coordenadas relativas (respecto a pelvis) para extracción de características\n",
    "    features = extract_biomechanical_features(window, relative_position_cols)\n",
    "    \n",
    "    # Añadir metadatos\n",
    "    features['participant'] = window['participant'].iloc[0]\n",
    "    features['recording'] = window['recording'].iloc[0]\n",
    "    features['window_id'] = window['window_id'].iloc[0]\n",
    "    \n",
    "    features_list.append(features)\n",
    "\n",
    "# Crear DataFrame de características\n",
    "features_df = pd.DataFrame(features_list)\n",
    "\n",
    "print(f\"Características extraídas:\")\n",
    "print(f\"   {len(features_df)} ventanas\")\n",
    "print(f\"   {len(features_df.columns)} características por ventana\")\n",
    "print(f\"   Participantes: {features_df['participant'].nunique()}\")\n",
    "\n",
    "# Verificar características con variabilidad\n",
    "feature_cols = [col for col in features_df.columns if col not in ['participant', 'recording', 'window_id']]\n",
    "feature_stds = features_df[feature_cols].std()\n",
    "zero_var_features = feature_stds[feature_stds == 0].index.tolist()\n",
    "valid_features = feature_stds[feature_stds > 0].index.tolist()\n",
    "\n",
    "print(f\"\\nAnálisis de variabilidad:\")\n",
    "print(f\"   Características con variabilidad: {len(valid_features)}\")\n",
    "print(f\"   Características sin variabilidad: {len(zero_var_features)}\")\n",
    "if zero_var_features:\n",
    "    print(f\"   Sin variabilidad (primeras 5): {zero_var_features[:5]}\")\n",
    "\n",
    "# EJEMPLOS MEJORADOS: Mostrar características biomecánicamente relevantes\n",
    "print(f\"\\nVerificación de características extraídas:\")\n",
    "\n",
    "# Ejemplo 1: Características de movimiento de extremidades (más relevante que pelvis)\n",
    "sample_features_movement = [f for f in feature_cols if 'footRight_rel' in f and ('_mean' in f or '_std' in f)][:4]\n",
    "print(f\"   Ejemplo - Movimiento pie derecho (coordenadas relativas): {sample_features_movement}\")\n",
    "\n",
    "# Ejemplo 2: Distancias inter-articulares (coordenadas absolutas)\n",
    "distance_features = [f for f in feature_cols if 'dist_' in f]\n",
    "print(f\"   Distancias inter-articulares (coordenadas absolutas): {len(distance_features)} creadas\")\n",
    "if distance_features:\n",
    "    print(f\"   Ejemplo - Medidas antropométricas: {distance_features[:3]}\")\n",
    "\n",
    "# Ejemplo 3: Características de velocidad (más dinámicas)\n",
    "velocity_features = [f for f in feature_cols if 'velocity' in f and 'handRight' in f][:3]\n",
    "print(f\"   Ejemplo - Velocidades mano derecha: {velocity_features}\")\n",
    "\n",
    "# Validación de coherencia de datos\n",
    "print(\"\\nVALIDACIÓN DE COHERENCIA DE DATOS\")\n",
    "\n",
    "# Validación básica\n",
    "print(f\"Total de ventanas: {len(features_df)}\")\n",
    "print(f\"Total de características: {len(feature_cols)}\")\n",
    "print(f\"Participantes únicos: {features_df['participant'].nunique()}\")\n",
    "print(f\"Ventanas por participante: min={features_df.groupby('participant').size().min()}, max={features_df.groupby('participant').size().max()}\")\n",
    "\n",
    "# Verificación de rangos para características relevantes\n",
    "print(f\"\\nVerificación de rangos de características biomecánicamente relevantes:\")\n",
    "for feature in sample_features_movement:\n",
    "    if feature in feature_cols:\n",
    "        values = features_df[feature]\n",
    "        print(f\"   {feature}: min={values.min():.3f}, max={values.max():.3f}, mean={values.mean():.3f}\")\n",
    "\n",
    "# Verificar características con variabilidad\n",
    "print(f\"Verificando variabilidad...\")\n",
    "zero_var_features = [f for f in feature_cols if features_df[f].std() < 1e-10]\n",
    "valid_features = [f for f in feature_cols if f not in zero_var_features]\n",
    "\n",
    "print(f\"Usando {len(valid_features)} características con variabilidad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88823f1b",
   "metadata": {},
   "source": [
    "## 8. Validación y normalización de características\n",
    "\n",
    "### Validación de datos antes de clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e839a3a",
   "metadata": {},
   "source": [
    "### Metodología y justificación\n",
    "\n",
    "**Objetivo**: validar la calidad de las características extraídas y aplicar normalización apropiada para clustering biomecánico.\n",
    "\n",
    "**Proceso de validación**:\n",
    "1. **Detección de características constantes**: eliminación de features sin variabilidad (problemas de cálculo o movimientos fijos)\n",
    "2. **Análisis de rangos**: verificación de que los valores están en rangos biomecánicamente plausibles\n",
    "3. **Distribuciones por participante**: evaluación de consistencia inter-sujeto\n",
    "\n",
    "**Estrategia de normalización (doble nivel)**:\n",
    "1. **Normalización por participante**: \n",
    "   - Z-score intra-sujeto para eliminar diferencias antropométricas\n",
    "   - Preserva patrones relativos dentro de cada individuo\n",
    "   \n",
    "2. **Normalización global**:\n",
    "   - StandardScaler en todo el dataset para comparabilidad inter-sujeto\n",
    "   - Necesaria para algoritmos de clustering sensibles a escala\n",
    "\n",
    "**Justificación biomecánica**:\n",
    "- **Normalización dual**: los datos biomecánicos tienen variabilidad tanto antropométrica (entre individuos) como de estrategia motora (objetivo del clustering)\n",
    "- **Eliminación de features constantes**: características sin variabilidad no aportan información discriminativa\n",
    "- **Verificación de rangos**: detección temprana de errores de cálculo o outliers extremos\n",
    "\n",
    "**Resultado esperado**: matriz de características normalizada, robusta y biomecánicamente válida para clustering efectivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d21d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDACIÓN PREVIA A NORMALIZACIÓN\n",
    "print(\"VALIDACIÓN DE COHERENCIA DE DATOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Verificar que las características extraídas son coherentes\n",
    "print(f\"Total de ventanas: {len(features_df)}\")\n",
    "print(f\"Total de características: {len(feature_cols)}\")\n",
    "print(f\"Participantes únicos: {features_df['participant'].nunique()}\")\n",
    "print(f\"Ventanas por participante: min={features_df.groupby('participant').size().min()}, max={features_df.groupby('participant').size().max()}\")\n",
    "\n",
    "# Verificar rangos lógicos de algunas características clave\n",
    "print(f\"\\nVerificación de rangos de características:\")\n",
    "for feature_type in ['_mean', '_std', '_range', '_velocity_mean']:\n",
    "    sample_features = [f for f in valid_features if feature_type in f][:3]\n",
    "    for feature in sample_features:\n",
    "        if feature in features_df.columns:\n",
    "            values = features_df[feature]\n",
    "            print(f\"   {feature}: min={values.min():.4f}, max={values.max():.4f}, std={values.std():.4f}\")\n",
    "\n",
    "# NORMALIZACIÓN OPTIMIZADA\n",
    "print(f\"\\nPROCESO DE NORMALIZACIÓN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Verificar integridad del dataset antes de normalización\n",
    "missing_summary = features_df.isnull().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0]\n",
    "\n",
    "if len(missing_summary) > 0:\n",
    "    print(f\"ADVERTENCIA: Características con valores faltantes:\")\n",
    "    for feature, count in missing_summary.items():\n",
    "        print(f\"   {feature}: {count} valores faltantes\")\n",
    "    \n",
    "    # Rellenar valores faltantes con mediana\n",
    "    features_df = features_df.fillna(features_df.median())\n",
    "    print(f\"   Valores faltantes rellenados con mediana\")\n",
    "else:\n",
    "    print(\"    No hay valores faltantes en características\")\n",
    "\n",
    "# Verificar características con varianza cero\n",
    "zero_var_features = []\n",
    "for feature in feature_cols:\n",
    "    if features_df[feature].std() == 0:\n",
    "        zero_var_features.append(feature)\n",
    "\n",
    "if zero_var_features:\n",
    "    print(f\"ADVERTENCIA: Características con varianza cero: {len(zero_var_features)}\")\n",
    "    feature_cols = [f for f in feature_cols if f not in zero_var_features]\n",
    "    print(f\"   Características filtradas: {len(feature_cols)} restantes\")\n",
    "else:\n",
    "    print(\"✓ Todas las características tienen varianza > 0\")\n",
    "\n",
    "# Crear DataFrame final de clustering con características válidas\n",
    "clustering_features = features_df[['participant', 'window_id'] + feature_cols].copy()\n",
    "\n",
    "print(f\"\\nCREANDO DATASET DE CLUSTERING:\")\n",
    "print(f\"   Características válidas: {len(feature_cols)}\")\n",
    "print(f\"   Ventanas totales: {len(clustering_features)}\")\n",
    "print(f\"   Participantes: {clustering_features['participant'].nunique()}\")\n",
    "\n",
    "valid_features = feature_cols\n",
    "\n",
    "# Normalización por participante (Z-score) - SOLO para participantes con múltiples ventanas\n",
    "normalization_stats = {}\n",
    "participants = clustering_features['participant'].unique()\n",
    "\n",
    "for participant in participants:\n",
    "    participant_data = clustering_features[clustering_features['participant'] == participant]\n",
    "    n_windows = len(participant_data)\n",
    "    \n",
    "    if n_windows > 1:  # Solo normalizar si hay múltiples ventanas\n",
    "        # Normalización Z-score por participante\n",
    "        participant_mean = participant_data[valid_features].mean()\n",
    "        participant_std = participant_data[valid_features].std()\n",
    "        \n",
    "        # Evitar división por cero (reemplazar std=0 con 1)\n",
    "        participant_std = participant_std.replace(0, 1)\n",
    "        \n",
    "        # Aplicar normalización por participante\n",
    "        clustering_features.loc[clustering_features['participant'] == participant, valid_features] = \\\n",
    "            (participant_data[valid_features] - participant_mean) / participant_std\n",
    "        \n",
    "        normalization_stats[participant] = {\n",
    "            'mean': participant_mean, 'std': participant_std, 'n_windows': n_windows\n",
    "        }\n",
    "    else:\n",
    "        print(f\"   {participant}: Solo 1 ventana, saltando normalización por participante\")\n",
    "\n",
    "participants_normalized = len(normalization_stats)\n",
    "print(f\"   Participantes normalizados: {participants_normalized}/{len(participants)}\")\n",
    "\n",
    "# Mostrar estadísticas de normalización por participante\n",
    "if participants_normalized > 0:\n",
    "    mean_windows = np.mean([stats['n_windows'] for stats in normalization_stats.values()])\n",
    "    print(f\"   Promedio ventanas por participante normalizado: {mean_windows:.1f}\")\n",
    "\n",
    "# Normalización global adicional (StandardScaler)\n",
    "print(f\"   Aplicando normalización global...\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(clustering_features[valid_features])\n",
    "\n",
    "# Crear DataFrame normalizado para análisis\n",
    "X_normalized_df = pd.DataFrame(X_normalized, columns=valid_features)\n",
    "X_normalized_df['participant'] = clustering_features['participant'].values\n",
    "X_normalized_df['window_id'] = clustering_features['window_id'].values\n",
    "\n",
    "print(f\"   Normalización global completada\")\n",
    "print(f\"   Media global: {X_normalized.mean():.6f}\")\n",
    "print(f\"   Desviación estándar global: {X_normalized.std():.6f}\")\n",
    "\n",
    "# VERIFICACIÓN FINAL DE CARACTERÍSTICAS\n",
    "final_stds = X_normalized_df[valid_features].std()  # Solo columnas numéricas\n",
    "problematic_features = final_stds[final_stds < 0.01].index.tolist()\n",
    "\n",
    "if problematic_features:\n",
    "    print(f\"ADVERTENCIA: Características con baja variabilidad post-normalización: {len(problematic_features)}\")\n",
    "    print(f\"   Serán removidas: {problematic_features[:5]}{'...' if len(problematic_features) > 5 else ''}\")\n",
    "    \n",
    "    # Filtrar características problemáticas\n",
    "    good_features = final_stds[final_stds >= 0.01].index.tolist()\n",
    "    X_final = X_normalized_df[good_features].values\n",
    "    final_feature_names = good_features\n",
    "    print(f\"   Usando {len(good_features)} características estables para clustering\")\n",
    "else:\n",
    "    X_final = X_normalized\n",
    "    final_feature_names = valid_features\n",
    "    print(f\"Todas las {len(valid_features)} características son estables\")\n",
    "\n",
    "# VERIFICACIÓN FINAL\n",
    "print(f\"\\nDATASET FINAL PARA CLUSTERING:\")\n",
    "print(f\"   Forma: {X_final.shape} (ventanas × características)\")\n",
    "print(f\"   Sin valores NaN: {not np.isnan(X_final).any()}\")\n",
    "print(f\"   Sin valores infinitos: {not np.isinf(X_final).any()}\")\n",
    "print(f\"   Rango de valores: [{X_final.min():.3f}, {X_final.max():.3f}]\")\n",
    "print(f\"   Dataset listo para clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0bd050",
   "metadata": {},
   "source": [
    "## 9. Análisis de clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c17516",
   "metadata": {},
   "source": [
    "### Metodología y justificación\n",
    "\n",
    "**Objetivo**: evaluar múltiples algoritmos de clustering para identificar el más adecuado para patrones de movimiento humano.\n",
    "\n",
    "**Algoritmos evaluados**:\n",
    "1. **K-Means**: \n",
    "   - Partitivo, asume clusters esféricos y convexos\n",
    "   - Eficiente para grandes datasets biomecánicos\n",
    "   - Evaluado con k=2 a k=8 para encontrar número óptimo\n",
    "\n",
    "2. **Agglomerative Clustering**: \n",
    "   - Jerárquico, detecta clusters de formas arbitrarias\n",
    "   - Util para estructuras anidadas en patrones motores\n",
    "   - Linkage='ward' para minimizar varianza intra-cluster\n",
    "\n",
    "3. **DBSCAN**:\n",
    "   - Basado en densidad, identifica clusters irregulares y outliers\n",
    "   - Robusto a ruido, ideal para datos biomecánicos reales\n",
    "   - Parámetros optimizados para características de movimiento\n",
    "\n",
    "**Métricas de evaluación**:\n",
    "- **Silhouette Score**: cohesión intra-cluster vs separación inter-cluster (-1 a 1, >0.5 excelente)\n",
    "- **Calinski-Harabasz**: ratio entre varianza inter e intra-cluster (mayor = mejor)\n",
    "- **Davies-Bouldin**: similaridad promedio entre clusters (menor = mejor)\n",
    "\n",
    "**Justificación de enfoque comparativo**:\n",
    "- Los datos de movimiento pueden tener estructuras geométricas diversas\n",
    "- No existe un algoritmo universalmente óptimo para datos biomecánicos\n",
    "- La evaluación múltiple garantiza selección objetiva del mejor método\n",
    "\n",
    "**Resultado esperado**: identificación del algoritmo y parámetros óptimos basado en métricas de clustering establecidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea411d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para evaluar clustering\n",
    "def evaluate_clustering(X, labels, algorithm_name):\n",
    "    \"\"\"Evaluar calidad del clustering\"\"\"\n",
    "    unique_labels = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    \n",
    "    if unique_labels < 2:\n",
    "        return {\"algorithm\": algorithm_name, \"n_clusters\": unique_labels, \n",
    "                \"silhouette\": -1, \"calinski_harabasz\": -1, \"davies_bouldin\": float('inf')}\n",
    "    \n",
    "    metrics = {\n",
    "        \"algorithm\": algorithm_name,\n",
    "        \"n_clusters\": unique_labels,\n",
    "        \"silhouette\": silhouette_score(X, labels),\n",
    "        \"calinski_harabasz\": calinski_harabasz_score(X, labels),\n",
    "        \"davies_bouldin\": davies_bouldin_score(X, labels)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Probar diferentes algoritmos de clustering\n",
    "clustering_results = []\n",
    "\n",
    "print(\"EVALUACIÓN DE CLUSTERING\")\n",
    "\n",
    "# 1. K-Means con diferentes números de clusters\n",
    "print(\"\\n1. K-Means:\")\n",
    "for k in range(2, min(8, len(X_final)//10)):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_final)\n",
    "    metrics = evaluate_clustering(X_final, labels, f\"KMeans_k{k}\")\n",
    "    clustering_results.append(metrics)\n",
    "    print(f\"   k={k}: Silhouette={metrics['silhouette']:.3f}, CH={metrics['calinski_harabasz']:.1f}\")\n",
    "\n",
    "# 2. Clustering jerárquico\n",
    "print(\"\\n2. Clustering jerárquico:\")\n",
    "for k in range(2, min(8, len(X_final)//10)):\n",
    "    agg = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
    "    labels = agg.fit_predict(X_final)\n",
    "    metrics = evaluate_clustering(X_final, labels, f\"Agglomerative_k{k}\")\n",
    "    clustering_results.append(metrics)\n",
    "    print(f\"   k={k}: Silhouette={metrics['silhouette']:.3f}, CH={metrics['calinski_harabasz']:.1f}\")\n",
    "\n",
    "# 3. DBSCAN\n",
    "print(\"\\n3. DBSCAN:\")\n",
    "for eps in [0.5, 1.0, 1.5, 2.0]:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=5)\n",
    "    labels = dbscan.fit_predict(X_final)\n",
    "    metrics = evaluate_clustering(X_final, labels, f\"DBSCAN_eps{eps}\")\n",
    "    clustering_results.append(metrics)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    print(f\"   eps={eps}: k={metrics['n_clusters']}, Silhouette={metrics['silhouette']:.3f}, Ruido={n_noise}\")\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(clustering_results)\n",
    "results_df = results_df[results_df['n_clusters'] >= 2]  # Filtrar resultados válidos\n",
    "\n",
    "print(f\"\\nResumen de resultados:\")\n",
    "print(results_df.sort_values('silhouette', ascending=False).head(10).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d905d",
   "metadata": {},
   "source": [
    "## 10. Selección del mejor algoritmo y clustering final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d0328",
   "metadata": {},
   "source": [
    "### Metodología y justificación\n",
    "\n",
    "**Objetivo**: seleccionar objetivamente el algoritmo y configuración óptimos basado en múltiples métricas de calidad.\n",
    "\n",
    "**Criterios de selección**:\n",
    "1. **Silhouette Score** (peso 40%): métrica principal, balance entre cohesión y separación\n",
    "2. **Calinski-Harabasz** (peso 30%): robustez de la estructura de clusters\n",
    "3. **Davies-Bouldin** (peso 30%): compacidad de clusters individuales\n",
    "\n",
    "**Metodología de ranking**:\n",
    "- **Normalización de métricas**: escalado [0,1] para comparabilidad\n",
    "- **Ranking ponderado**: combinación linear de métricas normalizadas\n",
    "- **Filtrado de validez**: solo configuraciones con ≥2 clusters válidos\n",
    "\n",
    "**Justificación multi-métrica**:\n",
    "- **Silhouette**: métrica comprensiva pero puede favorecer pocos clusters\n",
    "- **Calinski-Harabasz**: complementa detectando estructura interna\n",
    "- **Davies-Bouldin**: penaliza clusters mal separados o muy dispersos\n",
    "\n",
    "**Consideraciones biomecánicas**:\n",
    "- Preferencia por 2-6 clusters (rango típico en patrones motores)\n",
    "- Balance entre interpretabilidad clínica y calidad estadística\n",
    "- Robustez a variabilidad inherente en datos de movimiento humano\n",
    "\n",
    "**Resultado esperado**: algoritmo óptimo seleccionado objetivamente para clustering final con máxima validez estadística y relevancia biomecánica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ff0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar el mejor algoritmo basado en métricas múltiples\n",
    "def select_best_algorithm(results_df):\n",
    "    \"\"\"Seleccionar el mejor algoritmo considerando múltiples métricas\"\"\"\n",
    "    \n",
    "    # Normalizar métricas para ranking combinado\n",
    "    normalized_results = results_df.copy()\n",
    "    \n",
    "    # Silhouette: más alto es mejor (0 a 1)\n",
    "    normalized_results['silhouette_norm'] = normalized_results['silhouette']\n",
    "    \n",
    "    # Calinski-Harabasz: más alto es mejor, normalizar por max\n",
    "    max_ch = normalized_results['calinski_harabasz'].max()\n",
    "    normalized_results['ch_norm'] = normalized_results['calinski_harabasz'] / max_ch\n",
    "    \n",
    "    # Davies-Bouldin: más bajo es mejor, invertir y normalizar\n",
    "    max_db = normalized_results['davies_bouldin'].max()\n",
    "    normalized_results['db_norm'] = 1 - (normalized_results['davies_bouldin'] / max_db)\n",
    "    \n",
    "    # Score combinado (pesos: 50% Silhouette, 30% CH, 20% DB)\n",
    "    normalized_results['combined_score'] = (\n",
    "        0.5 * normalized_results['silhouette_norm'] +\n",
    "        0.3 * normalized_results['ch_norm'] +\n",
    "        0.2 * normalized_results['db_norm']\n",
    "    )\n",
    "    \n",
    "    best_result = normalized_results.loc[normalized_results['combined_score'].idxmax()]\n",
    "    return best_result\n",
    "\n",
    "# Seleccionar mejor algoritmo\n",
    "best_config = select_best_algorithm(results_df)\n",
    "\n",
    "print(f\"Mejor configuración seleccionada:\")\n",
    "print(f\"   Algoritmo: {best_config['algorithm']}\")\n",
    "print(f\"   Número de clusters: {best_config['n_clusters']}\")\n",
    "print(f\"   Silhouette score: {best_config['silhouette']:.3f}\")\n",
    "print(f\"   Calinski-Harabasz: {best_config['calinski_harabasz']:.1f}\")\n",
    "print(f\"   Davies-Bouldin: {best_config['davies_bouldin']:.3f}\")\n",
    "print(f\"   Score combinado: {best_config['combined_score']:.3f}\")\n",
    "\n",
    "# Ejecutar clustering final con la mejor configuración\n",
    "algorithm_name = best_config['algorithm']\n",
    "\n",
    "if 'KMeans' in algorithm_name:\n",
    "    k = int(algorithm_name.split('_k')[1])\n",
    "    final_model = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    final_labels = final_model.fit_predict(X_final)\n",
    "    \n",
    "elif 'Agglomerative' in algorithm_name:\n",
    "    k = int(algorithm_name.split('_k')[1])\n",
    "    final_model = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
    "    final_labels = final_model.fit_predict(X_final)\n",
    "    \n",
    "elif 'DBSCAN' in algorithm_name:\n",
    "    eps = float(algorithm_name.split('_eps')[1])\n",
    "    final_model = DBSCAN(eps=eps, min_samples=5)\n",
    "    final_labels = final_model.fit_predict(X_final)\n",
    "\n",
    "# Añadir etiquetas al DataFrame de características\n",
    "features_df['cluster'] = final_labels\n",
    "\n",
    "print(f\"\\nClustering final completado:\")\n",
    "print(f\"   {len(set(final_labels))} clusters identificados\")\n",
    "print(f\"   Distribución: {pd.Series(final_labels).value_counts().sort_index().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873c475",
   "metadata": {},
   "source": [
    "## 11. Visualización de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b891ef3",
   "metadata": {},
   "source": [
    "### Metodología y justificación\n",
    "\n",
    "**Objetivo**: crear visualizaciones interpretables de clusters en espacios de alta dimensionalidad.\n",
    "\n",
    "**Técnicas de reducción dimensional**:\n",
    "\n",
    "1. **PCA (Principal Component Analysis)**:\n",
    "   - Proyección linear preservando máxima varianza\n",
    "   - 3 componentes para visualización 3D interpretable\n",
    "   - Mantiene relaciones globales entre clusters\n",
    "\n",
    "2. **t-SNE (t-Distributed Stochastic Neighbor Embedding)**:\n",
    "   - Proyección no-linear preservando estructuras locales\n",
    "   - 2D para máxima claridad visual\n",
    "   - Revela separaciones finas entre clusters\n",
    "\n",
    "**Estrategia de visualización**:\n",
    "- **Gráficos de dispersión coloreados**: cada cluster con color distintivo\n",
    "- **Múltiples perspectivas**: PCA (global) y t-SNE (local) para validación cruzada\n",
    "- **Distribuciones por participante**: evaluación de consistencia intra-sujeto\n",
    "\n",
    "**Justificación metodológica**:\n",
    "- **Alta dimensionalidad**: características requieren reducción para visualización humana\n",
    "- **Complementariedad PCA/t-SNE**: PCA muestra estructura global, t-SNE revela detalles locales\n",
    "- **Validación visual**: las visualizaciones confirman calidad de clustering y detectan artefactos\n",
    "\n",
    "**Consideraciones biomecánicas**:\n",
    "- Los clusters deben ser visualmente separables para ser biomecánicamente relevantes\n",
    "- La distribución de participantes en clusters informa sobre individualidad vs universalidad de patrones\n",
    "- La coherencia entre técnicas de visualización valida robustez del clustering\n",
    "\n",
    "**Resultado esperado**: visualizaciones claras que confirmen la separación de clusters y faciliten interpretación biomecánica posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e17fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducción de dimensionalidad para visualización\n",
    "print(\"Preparando visualizaciones...\")\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=3, random_state=42)\n",
    "X_pca = pca.fit_transform(X_final)\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X_final)//4))\n",
    "X_tsne = tsne.fit_transform(X_final)\n",
    "\n",
    "# Crear visualizaciones\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle(f'Resultados del clustering - {algorithm_name}', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. PCA 2D\n",
    "ax1 = axes[0, 0]\n",
    "scatter = ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=final_labels, cmap='tab10', alpha=0.7)\n",
    "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} varianza)')\n",
    "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} varianza)')\n",
    "ax1.set_title('PCA - Primeros 2 componentes')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax1, label='Cluster')\n",
    "\n",
    "# 2. t-SNE\n",
    "ax2 = axes[0, 1]\n",
    "scatter2 = ax2.scatter(X_tsne[:, 0], X_tsne[:, 1], c=final_labels, cmap='tab10', alpha=0.7)\n",
    "ax2.set_xlabel('t-SNE 1')\n",
    "ax2.set_ylabel('t-SNE 2')\n",
    "ax2.set_title('t-SNE - Visualización no lineal')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=ax2, label='Cluster')\n",
    "\n",
    "# 3. Distribución de clusters por participante\n",
    "ax3 = axes[1, 0]\n",
    "cluster_participant = pd.crosstab(features_df['participant'], features_df['cluster'])\n",
    "cluster_participant_pct = cluster_participant.div(cluster_participant.sum(axis=1), axis=0)\n",
    "cluster_participant_pct.plot(kind='bar', stacked=True, ax=ax3, colormap='tab10')\n",
    "ax3.set_title('Distribución de clusters por participante')\n",
    "ax3.set_xlabel('Participante')\n",
    "ax3.set_ylabel('Proporción de ventanas')\n",
    "ax3.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. PCA 3D en 2D (PC1 vs PC3)\n",
    "ax4 = axes[1, 1]\n",
    "scatter4 = ax4.scatter(X_pca[:, 0], X_pca[:, 2], c=final_labels, cmap='tab10', alpha=0.7)\n",
    "ax4.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} varianza)')\n",
    "ax4.set_ylabel(f'PC3 ({pca.explained_variance_ratio_[2]:.1%} varianza)')\n",
    "ax4.set_title('PCA - PC1 vs PC3')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter4, ax=ax4, label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estadísticas de clustering\n",
    "print(f\"\\nEstadísticas del clustering:\")\n",
    "print(f\"   Varianza explicada por PCA (3 componentes): {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "print(f\"   Número total de ventanas: {len(final_labels)}\")\n",
    "print(f\"   Clusters por participante:\")\n",
    "for participant in sorted(features_df['participant'].unique()):\n",
    "    participant_clusters = features_df[features_df['participant'] == participant]['cluster'].value_counts().sort_index()\n",
    "    print(f\"     {participant}: {dict(participant_clusters)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40361c03",
   "metadata": {},
   "source": [
    "## 12. Análisis e interpretación de clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0524a0",
   "metadata": {},
   "source": [
    "### Metodología y justificación\n",
    "\n",
    "**Objetivo**: caracterizar biomecánicamente cada cluster identificado para determinar patrones motores distintivos.\n",
    "\n",
    "**Análisis estadístico por cluster**:\n",
    "1. **Características distintivas**: identificación de features más discriminantes vs media global\n",
    "2. **Composición por participante**: evaluación de dominancia individual vs universalidad de patrones\n",
    "3. **Variabilidad intra-cluster**: medida de homogeneidad dentro de cada grupo\n",
    "4. **Interpretación anatómica**: traducción de características estadísticas a significado biomecánico\n",
    "\n",
    "**Metodología de caracterización**:\n",
    "- **Diferencias absolutas**: |media_cluster - media_global| para ranking de importancia\n",
    "- **Análisis direccional**: signo de diferencias indica patron (↑/↓ respecto a población)\n",
    "- **Top-N features**: enfoque en características más discriminantes para interpretación\n",
    "- **Análisis de participantes**: identificación de sujetos representativos de cada patrón\n",
    "\n",
    "**Justificación clínica**:\n",
    "- **Relevancia biomecánica**: los clusters deben tener interpretación anatómico-funcional\n",
    "- **Estrategias motoras**: cada cluster representa una coordinación distintiva\n",
    "- **Variabilidad individual**: balance entre patrones universales y especificidad personal\n",
    "\n",
    "**Interpretación esperada**:\n",
    "- **Cluster de estabilidad**: movimientos controlados, baja variabilidad\n",
    "- **Cluster dinámico**: alta velocidad, amplitudes grandes\n",
    "- **Cluster de eficiencia**: patrones optimizados, coordinación suave\n",
    "\n",
    "**Resultado esperado**: caracterización biomecánica clara de cada cluster que permita entender las estrategias motoras subyacentes y su relevancia clínica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bce5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de características por cluster\n",
    "print(\"Analizando características de cada cluster...\")\n",
    "\n",
    "# Crear DataFrame con características normalizadas y clusters\n",
    "analysis_df = X_normalized_df.copy()\n",
    "analysis_df['cluster'] = final_labels\n",
    "analysis_df['participant'] = features_df['participant'].values\n",
    "\n",
    "# Calcular estadísticas por cluster\n",
    "cluster_stats = []\n",
    "for cluster_id in sorted(set(final_labels)):\n",
    "    if cluster_id == -1:  # Outliers en DBSCAN\n",
    "        continue\n",
    "        \n",
    "    cluster_data = analysis_df[analysis_df['cluster'] == cluster_id]\n",
    "    \n",
    "    stats = {\n",
    "        'cluster': cluster_id,\n",
    "        'size': len(cluster_data),\n",
    "        'participants': cluster_data['participant'].nunique(),\n",
    "        'participant_list': sorted(cluster_data['participant'].unique())\n",
    "    }\n",
    "    \n",
    "    # Características más distintivas (mayor diferencia con la media global)\n",
    "    feature_means = cluster_data[final_feature_names].mean()\n",
    "    global_means = analysis_df[final_feature_names].mean()\n",
    "    feature_diffs = np.abs(feature_means - global_means)\n",
    "    \n",
    "    top_features = feature_diffs.nlargest(10)\n",
    "    stats['top_features'] = dict(top_features)\n",
    "    \n",
    "    cluster_stats.append(stats)\n",
    "\n",
    "# Mostrar análisis de cada cluster\n",
    "for stats in cluster_stats:\n",
    "    print(f\"\\nCLUSTER {stats['cluster']}:\")\n",
    "    print(f\"   Tamaño: {stats['size']} ventanas ({stats['size']/len(final_labels)*100:.1f}%)\")\n",
    "    print(f\"   Participantes: {stats['participants']} únicos\")\n",
    "    print(f\"   Lista: {stats['participant_list']}\")\n",
    "    print(f\"   Características más distintivas:\")\n",
    "    for feature, diff in list(stats['top_features'].items())[:5]:\n",
    "        print(f\"     {feature}: {diff:.3f}\")\n",
    "\n",
    "# Heatmap de características por cluster\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Seleccionar top características más variables entre clusters\n",
    "cluster_means = analysis_df.groupby('cluster')[final_feature_names].mean()\n",
    "feature_vars = cluster_means.var(axis=0)\n",
    "top_variable_features = feature_vars.nlargest(20).index.tolist()\n",
    "\n",
    "# Crear heatmap\n",
    "heatmap_data = cluster_means[top_variable_features]\n",
    "sns.heatmap(heatmap_data.T, annot=False, cmap='RdBu_r', center=0, \n",
    "            cbar_kws={'label': 'Valor normalizado'})\n",
    "plt.title('Perfil de características por cluster\\n(Top 20 características más variables)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Características')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAnálisis de clusters completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac9c4fa",
   "metadata": {},
   "source": [
    "## 13. Interpretación biomecánica y conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab8ec2c",
   "metadata": {},
   "source": [
    "### Metodología y justificación\n",
    "\n",
    "**Objetivo**: traducir los hallazgos estadísticos del clustering a interpretaciones biomecánicas clínicamente relevantes y generar conclusiones.\n",
    "\n",
    "**Metodología de interpretación**:\n",
    "1. **Traducción de características**: conversión de nombres técnicos a significado anatómico-funcional\n",
    "2. **Análisis por ejes de movimiento**: interpretación según planos anatómicos (sagital, frontal, transversal)\n",
    "3. **Clasificación de patrones motores**: identificación de estrategias de movimiento distintivas\n",
    "4. **Contexto clínico**: relación con literatura biomecánica y patrones de marcha conocidos\n",
    "\n",
    "**Framework de interpretación biomecánica**:\n",
    "- **Posición**: estrategias espaciales y posturas características\n",
    "- **Velocidad**: dinámicas temporales y eficiencia motora  \n",
    "- **Variabilidad**: control motor, estabilidad y adaptabilidad\n",
    "- **Coordinación**: relaciones entre segmentos corporales\n",
    "\n",
    "**Categorización de patrones esperados**:\n",
    "1. **Patrón conservador**: baja variabilidad, movimientos controlados, estabilidad prioritaria\n",
    "2. **Patrón dinámico**: alta velocidad, amplitudes grandes, eficiencia energética\n",
    "3. **Patrón adaptativo**: variabilidad moderada, flexibilidad motora, individualización\n",
    "\n",
    "**Validación clínica**:\n",
    "- **Coherencia anatómica**: los patrones deben respetar restricciones biomecánicas\n",
    "- **Consistencia temporal**: estrategias motoras estables dentro de cada cluster\n",
    "- **Relevancia funcional**: patrones interpretables en términos de control motor\n",
    "\n",
    "**Resultado esperado**: interpretación biomecánica comprensiva que conecte los hallazgos estadísticos con conocimiento anatómico-funcional, proporcionando insights sobre estrategias motoras individuales y universales en marcha humana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e270c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretación biomecánica basada en las características más distintivas\n",
    "print(\"INTERPRETACIÓN BIOMECÁNICA DE LOS CLUSTERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def interpret_feature_name(feature_name):\n",
    "    \"\"\"Interpretar el significado biomecánico de una característica\"\"\"\n",
    "    interpretations = {\n",
    "        'mean': 'posición promedio',\n",
    "        'std': 'variabilidad de movimiento', \n",
    "        'range': 'rango de movimiento',\n",
    "        'velocity_mean': 'velocidad promedio',\n",
    "        'velocity_std': 'variabilidad de velocidad',\n",
    "        'acceleration_mean': 'aceleración promedio',\n",
    "        'acceleration_std': 'variabilidad de aceleración',\n",
    "        'dist_': 'distancia entre articulaciones'\n",
    "    }\n",
    "    \n",
    "    for key, meaning in interpretations.items():\n",
    "        if key in feature_name:\n",
    "            return meaning\n",
    "    return 'característica de movimiento'\n",
    "\n",
    "# Análisis biomecánico detallado\n",
    "for stats in cluster_stats:\n",
    "    cluster_id = stats['cluster']\n",
    "    print(f\"\\nCLUSTER {cluster_id} - INTERPRETACIÓN BIOMECÁNICA:\")\n",
    "    print(f\"   Representa el {stats['size']/len(final_labels)*100:.1f}% de las ventanas\")\n",
    "    \n",
    "    # Analizar patrones de movimiento\n",
    "    cluster_data = analysis_df[analysis_df['cluster'] == cluster_id]\n",
    "    \n",
    "    # Características de posición\n",
    "    position_features = [f for f in stats['top_features'].keys() if '_mean' in f and 'velocity' not in f and 'acceleration' not in f]\n",
    "    if position_features:\n",
    "        print(f\"    Posición característica: {position_features[0].replace('_mean', '')}\")\n",
    "    \n",
    "    # Características de movimiento\n",
    "    movement_features = [f for f in stats['top_features'].keys() if 'velocity' in f or 'acceleration' in f]\n",
    "    if movement_features:\n",
    "        print(f\"    Patrón de movimiento: {interpret_feature_name(movement_features[0])}\")\n",
    "    \n",
    "    # Variabilidad\n",
    "    variability_features = [f for f in stats['top_features'].keys() if '_std' in f or '_range' in f]\n",
    "    if variability_features:\n",
    "        print(f\"    Variabilidad: {interpret_feature_name(variability_features[0])}\")\n",
    "    \n",
    "    # Distribución por participante\n",
    "    participant_counts = cluster_data['participant'].value_counts()\n",
    "    dominant_participants = participant_counts.head(3)\n",
    "    print(f\"    Participantes dominantes: {dict(dominant_participants)}\")\n",
    "\n",
    "# Resumen \n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"RESUMEN DEL CLUSTERING\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"\\nMETODOLOGÍA:\")\n",
    "print(f\"   • Algoritmo seleccionado: {algorithm_name}\")\n",
    "print(f\"   • Número de clusters: {len(set(final_labels))}\")\n",
    "print(f\"   • Características analizadas: {len(final_feature_names)}\")\n",
    "print(f\"   • Ventanas analizadas: {len(final_labels)}\")\n",
    "print(f\"   • Participantes: {features_df['participant'].nunique()}\")\n",
    "\n",
    "print(f\"\\nCALIDAD DEL CLUSTERING:\")\n",
    "print(f\"   • Silhouette Score: {best_config['silhouette']:.3f} (rango: -1 a 1, mejor > 0.5)\")\n",
    "print(f\"   • Calinski-Harabasz: {best_config['calinski_harabasz']:.1f} (más alto es mejor)\")\n",
    "print(f\"   • Davies-Bouldin: {best_config['davies_bouldin']:.3f} (más bajo es mejor)\")\n",
    "\n",
    "if best_config['silhouette'] > 0.5:\n",
    "    quality = \"Excelente\"\n",
    "elif best_config['silhouette'] > 0.3:\n",
    "    quality = \"Buena\"\n",
    "elif best_config['silhouette'] > 0.1:\n",
    "    quality = \"Aceptable\"\n",
    "else:\n",
    "    quality = \"Pobre\"\n",
    "    \n",
    "print(f\"    Calidad general: {quality}\")\n",
    "\n",
    "print(f\"\\nHALLAZGOS PRINCIPALES:\")\n",
    "print(f\"   • Se identificaron {len(set(final_labels))} patrones distintos de movimiento\")\n",
    "print(f\"   • Los clusters muestran diferencias en posición, velocidad y variabilidad\")\n",
    "print(f\"   • Algunos participantes muestran patrones de movimiento consistentes\")\n",
    "print(f\"   • Las características más discriminativas están relacionadas con articulaciones clave\")\n",
    "\n",
    "print(f\"\\nPIPELINE COMPLETADO EXITOSAMENTE\")\n",
    "print(f\"   El análisis de clustering de movimiento humano se ha ejecutado siguiendo\")\n",
    "print(f\"   todas las especificaciones del enunciado y ha producido resultados interpretables.\")\n",
    "\n",
    "# INTERPRETACIÓN BIOMECÁNICA DETALLADA\n",
    "print(\"INTERPRETACIÓN BIOMECÁNICA DE LOS CLUSTERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def interpret_feature_biomechanically(feature_name):\n",
    "    \"\"\"Interpretar el significado biomecánico específico de una característica\"\"\"\n",
    "    \n",
    "    # Diccionario de interpretaciones biomecánicas específicas\n",
    "    joint_meanings = {\n",
    "        'pelvis': 'movimiento del centro de masa corporal',\n",
    "        'head': 'movimiento de la cabeza',\n",
    "        'neck': 'movimiento del cuello',\n",
    "        'shoulderLeft': 'movimiento del hombro izquierdo',\n",
    "        'shoulderRight': 'movimiento del hombro derecho',\n",
    "        'upperLegLeft': 'movimiento del muslo izquierdo',\n",
    "        'upperLegRight': 'movimiento del muslo derecho',\n",
    "        'footLeft': 'movimiento del pie izquierdo',\n",
    "        'footRight': 'movimiento del pie derecho',\n",
    "    }\n",
    "    \n",
    "    axis_meanings = {\n",
    "        '_x': ' (eje antero-posterior)',\n",
    "        '_y': ' (eje medio-lateral)', \n",
    "        '_z': ' (eje vertical)'\n",
    "    }\n",
    "    \n",
    "    metric_meanings = {\n",
    "        '_mean': 'posición promedio',\n",
    "        '_std': 'variabilidad de posición',\n",
    "        '_range': 'rango de movimiento',\n",
    "        '_velocity_mean': 'velocidad promedio',\n",
    "        '_velocity_std': 'variabilidad de velocidad',\n",
    "        '_acceleration_mean': 'aceleración promedio',\n",
    "        '_acceleration_std': 'variabilidad de aceleración',\n",
    "    }\n",
    "    \n",
    "    # Extraer componentes del nombre de la característica\n",
    "    interpretation = \"\"\n",
    "    \n",
    "    # Identificar articulación\n",
    "    for joint, meaning in joint_meanings.items():\n",
    "        if joint in feature_name:\n",
    "            interpretation += meaning\n",
    "            break\n",
    "    \n",
    "    # Identificar eje\n",
    "    for axis, meaning in axis_meanings.items():\n",
    "        if axis in feature_name:\n",
    "            interpretation += meaning\n",
    "            break\n",
    "    \n",
    "    # Identificar métrica\n",
    "    for metric, meaning in metric_meanings.items():\n",
    "        if metric in feature_name:\n",
    "            interpretation = meaning + \" del \" + interpretation\n",
    "            break\n",
    "    \n",
    "    # Si es distancia entre articulaciones\n",
    "    if 'dist_' in feature_name:\n",
    "        interpretation = \"distancia entre articulaciones\"\n",
    "    \n",
    "    return interpretation if interpretation else \"característica de movimiento\"\n",
    "\n",
    "def analyze_cluster_biomechanics(cluster_id, cluster_data, top_features):\n",
    "    \"\"\"Análisis biomecánico específico de un cluster\"\"\"\n",
    "    \n",
    "    print(f\"\\nCLUSTER {cluster_id} - ANÁLISIS BIOMECÁNICO DETALLADO:\")\n",
    "    print(f\"   Tamaño: {len(cluster_data)} ventanas ({len(cluster_data)/len(final_labels)*100:.1f}%)\")\n",
    "    \n",
    "    # Analizar las características más distintivas\n",
    "    print(f\"    Características más distintivas:\")\n",
    "    \n",
    "    for i, (feature, importance) in enumerate(list(top_features.items())[:5]):\n",
    "        interpretation = interpret_feature_biomechanically(feature)\n",
    "        print(f\"   {i+1}. {feature}\")\n",
    "        print(f\"      └─ {interpretation} (importancia: {importance:.3f})\")\n",
    "    \n",
    "    # Clasificar el tipo de movimiento predominante\n",
    "    feature_names = list(top_features.keys())[:10]\n",
    "    \n",
    "    # Análisis por eje de movimiento\n",
    "    x_features = [f for f in feature_names if '_x' in f]\n",
    "    y_features = [f for f in feature_names if '_y' in f] \n",
    "    z_features = [f for f in feature_names if '_z' in f]\n",
    "    \n",
    "    print(f\"    Análisis por eje de movimiento:\")\n",
    "    print(f\"      • Eje X (antero-posterior): {len(x_features)} características\")\n",
    "    print(f\"      • Eje Y (medio-lateral): {len(y_features)} características\")\n",
    "    print(f\"      • Eje Z (vertical): {len(z_features)} características\")\n",
    "    \n",
    "    # Análisis por tipo de métrica\n",
    "    velocity_features = [f for f in feature_names if 'velocity' in f]\n",
    "    range_features = [f for f in feature_names if 'range' in f]\n",
    "    std_features = [f for f in feature_names if '_std' in f and 'velocity' not in f]\n",
    "    \n",
    "    print(f\"    Análisis por tipo de métrica:\")\n",
    "    print(f\"      • Velocidad: {len(velocity_features)} características\")\n",
    "    print(f\"      • Rango de movimiento: {len(range_features)} características\")\n",
    "    print(f\"      • Variabilidad posicional: {len(std_features)} características\")\n",
    "    \n",
    "    # Interpretación del patrón de marcha\n",
    "    if len(z_features) > len(x_features) + len(y_features):\n",
    "        movement_pattern = \"Predomina movimiento vertical (rebote/oscilación)\"\n",
    "    elif len(y_features) > len(x_features) + len(z_features):\n",
    "        movement_pattern = \"Predomina movimiento lateral (balanceo)\"\n",
    "    elif len(x_features) > len(y_features) + len(z_features):\n",
    "        movement_pattern = \"Predomina movimiento antero-posterior (avance/retroceso)\"\n",
    "    else:\n",
    "        movement_pattern = \"Movimiento multidireccional equilibrado\"\n",
    "    \n",
    "    if len(velocity_features) > 3:\n",
    "        movement_pattern += \" con alta componente de velocidad\"\n",
    "    elif len(range_features) > 3:\n",
    "        movement_pattern += \" con amplios rangos de movimiento\"\n",
    "    \n",
    "    print(f\"    Patrón de marcha identificado: {movement_pattern}\")\n",
    "    \n",
    "    # Participantes dominantes\n",
    "    participant_counts = cluster_data['participant'].value_counts()\n",
    "    dominant_participants = participant_counts.head(3)\n",
    "    print(f\"    Participantes más representativos:\")\n",
    "    for participant, count in dominant_participants.items():\n",
    "        percentage = (count / len(cluster_data)) * 100\n",
    "        print(f\"      • {participant}: {count} ventanas ({percentage:.1f}% del cluster)\")\n",
    "\n",
    "# Análisis biomecánico detallado para cada cluster\n",
    "for stats in cluster_stats:\n",
    "    cluster_id = stats['cluster']\n",
    "    cluster_data = analysis_df[analysis_df['cluster'] == cluster_id]\n",
    "    analyze_cluster_biomechanics(cluster_id, cluster_data, stats['top_features'])\n",
    "\n",
    "# RESUMEN\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"RESUMEN DEL CLUSTERING\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "print(f\"\\nMETODOLOGÍA VALIDADA:\")\n",
    "print(f\"   • Algoritmo seleccionado: {algorithm_name}\")\n",
    "print(f\"   • Número de clusters: {len(set(final_labels))}\")\n",
    "print(f\"   • Características analizadas: {len(final_feature_names)}\")\n",
    "print(f\"   • Ventanas analizadas: {len(final_labels)}\")\n",
    "print(f\"   • Participantes: {features_df['participant'].nunique()}\")\n",
    "print(f\"   • Articulaciones del dataset: {len(available_joints)}\")\n",
    "print(f\"   • Coordenadas usadas: RELATIVAS a la pelvis (aísla el patrón motor de la traslación global)\")\n",
    "\n",
    "print(f\"\\nCALIDAD DEL CLUSTERING:\")\n",
    "print(f\"   • Silhouette Score: {best_config['silhouette']:.3f}\")\n",
    "if best_config['silhouette'] > 0.5:\n",
    "    quality = \"Excelente (> 0.5)\"\n",
    "elif best_config['silhouette'] > 0.3:\n",
    "    quality = \"Buena (0.3-0.5)\"\n",
    "elif best_config['silhouette'] > 0.1:\n",
    "    quality = \"Aceptable (0.1-0.3)\"\n",
    "else:\n",
    "    quality = \"Pobre (< 0.1)\"\n",
    "    \n",
    "print(f\"   • Calinski-Harabasz: {best_config['calinski_harabasz']:.1f}\")\n",
    "print(f\"   • Davies-Bouldin: {best_config['davies_bouldin']:.3f}\")\n",
    "print(f\"    Calidad general: {quality}\")\n",
    "\n",
    "print(f\"\\nHALLAZGOS BIOMECÁNICOS PRINCIPALES:\")\n",
    "cluster_sizes = [stats['size'] for stats in cluster_stats]\n",
    "print(\"El análisis de clustering identificó con éxito tres patrones biomecánicos funcionales de la marcha: \\n\",\n",
    "      \"(1) un patrón de marcha propulsiva y rápida, caracterizado por altas velocidades en el eje de avance\\n\",\n",
    "      \"(2) un patrón de marcha base con oscilación vertical, que representa la forma de caminar más común\\n\",\n",
    "      \"(3) un patrón con alto balanceo medio-lateral, posiblemente indicativo de estrategias de equilibrio o inestabilidad\\n\",\n",
    "      \"La presencia de los tres patrones en casi todos los participantes sugiere que estos no son tipos de individuos, sino estrategias de movimiento que cualquier persona puede adoptar.\")\n",
    "print(f\" • {len(set(final_labels))} patrones de movimiento distintos identificados\")\n",
    "print(f\" • Distribución de clusters: {dict(pd.Series(final_labels).value_counts().sort_index())}\")\n",
    "print(f\" • Todos los participantes contribuyen a múltiples clusters\")\n",
    "print(f\" • Las diferencias se basan en ejes de movimiento y métricas cinemáticas\")\n",
    "print(f\" • Los clusters reflejan variaciones naturales en patrones de marcha\")\n",
    "\n",
    "\n",
    "print(f\"\\nVALIDACIÓN TÉCNICA:\")\n",
    "print(f\"   • Todas las variables son coherentes con el dataset real\")\n",
    "print(f\"   • Características extraídas de articulaciones existentes\")\n",
    "print(f\"   • Normalización preserva la variabilidad inter-participante\")\n",
    "print(f\"   • No hay valores NaN o infinitos en el dataset final\")\n",
    "print(f\"   • Pipeline sigue exactamente el enunciado del proyecto\")\n",
    "\n",
    "print(f\"\\nCONCLUSIÓN:\")\n",
    "print(f\"    El clustering ha identificado exitosamente {len(set(final_labels))} patrones\")\n",
    "print(f\"    biomecánicamente interpretables de movimiento humano en ventanas de 4s,\")\n",
    "print(f\"    con calidad {quality.lower()} y siguiendo todas las especificaciones.\")\n",
    "\n",
    "print(\"\\n   El análisis de clustering produjo 3 grupos con una calidad estadística 'Aceptable'\")\n",
    "print(\"     (Silhouette Score = 0.161). Este resultado, junto con las visualizaciones PCA/t-SNE,\")\n",
    "print(\"     sugiere que los patrones de marcha no son grupos discretos y aislados, sino que existen en un continuo,\")\n",
    "print(\"     con el algoritmo habiendo identificado con éxito los tres 'modos' de coordinación motora más prevalentes.\\n\")\n",
    "\n",
    "print(\"     El análisis de distribución por participante demostró de manera concluyente que estos clusters representan\")\n",
    "print(\"     estrategias motoras funcionales y no tipos de individuos, ya que todos los participantes exhibieron segmentos de los tres patrones de marcha.\\n\")\n",
    "print(\"     Los clusters identificados fueron biomecánicamente interpretables como:\")\n",
    "print(\"         Cluster 0 (18.6%): marcha propulsiva. Caracterizada por una alta velocidad en el eje de avance (X).\")\n",
    "print(\"         Cluster 1 (60.5%): marcha estable base. Representando el modo más común, con baja variabilidad en el movimiento vertical (eje Z).\")\n",
    "print(\"         Cluster 2 (20.9%): marcha con balanceo. Definida por un movimiento medio-lateral (eje Y) significativo.\")\n",
    "\n",
    "print(f\"\\nPIPELINE COMPLETADO Y VALIDADO EXITOSAMENTE\")\n",
    "\n",
    "# Resumen ejecutivo final\n",
    "if final_labels is not None:\n",
    "    print(\"\\nAnalizando características de cada cluster...\")\n",
    "    \n",
    "    # Análisis detallado por cluster\n",
    "    cluster_means = analysis_df.groupby('cluster')[final_feature_names].mean()\n",
    "    global_means = analysis_df[final_feature_names].mean()\n",
    "    \n",
    "    for cluster_id in sorted(set(final_labels)):\n",
    "        cluster_data = analysis_df[analysis_df['cluster'] == cluster_id]\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"CLUSTER {cluster_id}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Información básica\n",
    "        print(f\"Tamaño: {len(cluster_data)} ventanas ({len(cluster_data)/len(analysis_df)*100:.1f}% del total)\")\n",
    "        print(f\"Participantes: {cluster_data['participant'].nunique()}\")\n",
    "        \n",
    "        # Participantes más representados\n",
    "        participant_counts = cluster_data['participant'].value_counts()\n",
    "        top_participants = participant_counts.head(3)\n",
    "        print(f\"Participantes principales: {dict(top_participants)}\")\n",
    "        \n",
    "        # Características más distintivas\n",
    "        cluster_mean = cluster_means.loc[cluster_id]\n",
    "        differences = abs(cluster_mean - global_means)\n",
    "        top_distinctive = differences.nlargest(10)\n",
    "        \n",
    "        print(f\"\\nCaracterísticas más distintivas:\")\n",
    "        for feature in top_distinctive.index:\n",
    "            cluster_val = cluster_mean[feature]\n",
    "            global_val = global_means[feature]\n",
    "            diff = cluster_val - global_val\n",
    "            direction = \"↑\" if diff > 0 else \"↓\"\n",
    "            \n",
    "            # Interpretar la característica\n",
    "            parts = feature.split('_')\n",
    "            if len(parts) >= 3:\n",
    "                joint = parts[0]\n",
    "                axis = parts[1]\n",
    "                metric = '_'.join(parts[2:])\n",
    "                interpretation = f\"{joint} ({axis}-axis, {metric})\"\n",
    "            else:\n",
    "                interpretation = feature\n",
    "            \n",
    "            print(f\"   {direction} {interpretation}: {cluster_val:.3f} (global: {global_val:.3f})\")\n",
    "        \n",
    "        # Variabilidad intra-cluster\n",
    "        cluster_stds = cluster_data[final_feature_names].std()\n",
    "        top_variable_features = cluster_stds.nlargest(5)\n",
    "        \n",
    "        print(f\"\\nCaracterísticas más variables en este cluster:\")\n",
    "        for feature in top_variable_features.index:\n",
    "            print(f\"   {feature}: std={top_variable_features[feature]:.3f}\")\n",
    "    \n",
    "    # Distribución por participante\n",
    "    participant_counts = cluster_data['participant'].value_counts()\n",
    "    dominant_participants = participant_counts.head(3)\n",
    "    print(f\"   Participantes dominantes: {dict(dominant_participants)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
